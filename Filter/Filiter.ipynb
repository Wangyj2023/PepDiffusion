{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from src.Transformer import MyDataset_class\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "with open('params/peptide_vocab.pkl', 'rb') as f:\n",
    "    w2i = pickle.load(f)\n",
    "import random\n",
    "\n",
    "def split_file(input_file, train_file, test_file, split_ratio=0.9):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "    split_point = int(len(lines) * split_ratio)\n",
    "    train_lines = lines[:split_point]\n",
    "    test_lines = lines[split_point:]\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(train_lines)\n",
    "    with open(test_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(test_lines)\n",
    "os.makedirs(f\"params/train\",exist_ok=True)\n",
    "os.makedirs(f\"params/test\",exist_ok=True)\n",
    "split_file(\"params/pos_data\", f\"params/train/pos\", f\"params/test/pos\")\n",
    "split_file(\"params/neg_data\", f\"params/train/neg\", f\"params/test/neg\")\n",
    "split_file(\"params/neg_data_10\", f\"params/train/neg_10\", f\"params/test/neg_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_path,negname,lr):\n",
    "    num_epochs = 300\n",
    "    os.makedirs(f\"./{save_path}\",exist_ok=True)\n",
    "    os.makedirs(f\"./{save_path}/model\",exist_ok=True)\n",
    "    all_data = MyDataset_class(f\"params/train/pos\",f\"params/train/{negname}\")\n",
    "    train_num = int(len(all_data) * 0.85)\n",
    "    val_num = int((len(all_data) - train_num))\n",
    "    train_set, val_set = random_split(all_data,[train_num, val_num])\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=512,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_set,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Train set     : {train_num}\")\n",
    "    print(f\"Val set       : {val_num}\")\n",
    "    print(f\"Learning_rate : {lr}\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.99)\n",
    "    best_acc_val = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            output = model(data.cuda())\n",
    "            loss = F.cross_entropy(output,labels.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        train_loss = np.array(losses).mean()\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        predict_all = np.array([], dtype=int)\n",
    "        labels_all = np.array([], dtype=int)\n",
    "        for i, (data, labels) in enumerate(val_loader):\n",
    "            output = model(data.cuda())\n",
    "            loss = F.cross_entropy(output,labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "            predic = torch.max(output.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "        val_loss = np.array(losses).mean()\n",
    "        val_acc = metrics.accuracy_score(labels_all,predict_all)\n",
    "        val_report = metrics.classification_report(labels_all, predict_all, target_names=[\"AMP\",\"NAMP\"], digits=4)\n",
    "        val_confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        # torch.save(model.state_dict(),f\"./{save_path}/model/{epoch}.pth\")\n",
    "        if val_acc > best_acc_val:\n",
    "            best_acc_val = val_acc\n",
    "            torch.save(model.state_dict(),f\"./{save_path}/best_model.pth\")\n",
    "            with open(f\"./{save_path}/model_data\",\"w\") as wf:\n",
    "                wf.write(f\"Epoch : {epoch}\\nTrain loss : {train_loss}\\nVal loss : {val_loss}\\n\")\n",
    "                wf.write(f\"************************  Val set  ************************\\n\")\n",
    "                wf.write(f\"{val_report}\")\n",
    "                wf.write(\"\\n\")\n",
    "                wf.write(f\"{val_confusion}\")\n",
    "                wf.write(\"\\n\")\n",
    "        print(f\"Epoch : {epoch}  Train loss : {train_loss}  Val loss : {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CNN import CNNModel\n",
    "from src.RCNN import RCNNModel\n",
    "from src.RNN_atten import RNN_attenModel\n",
    "from src.RNN import RNNModel\n",
    "from src.Transformer import TransformerModel, MyDataset_class\n",
    "\n",
    "run_lr = [0.01,0.001,0.0001,0.00001]\n",
    "\n",
    "for runlr in run_lr:\n",
    "    model = CNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-10/CNN_class_{runlr}\",\"neg_10\",runlr)\n",
    "    model = CNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-1/CNN_class_{runlr}\",\"neg\",runlr)\n",
    "\n",
    "    model = RCNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-10/RCNN_class_{runlr}\",\"neg_10\",runlr)\n",
    "    model = RCNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-1/RCNN_class_{runlr}\",\"neg\",runlr)\n",
    "\n",
    "    model = RNN_attenModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-10/RNN_atten_class_{runlr}\",\"neg_10\",runlr)\n",
    "    model = RNN_attenModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-1/RNN_atten_class_{runlr}\",\"neg\",runlr)\n",
    "\n",
    "    model = RNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-10/RNN_class_{runlr}\",\"neg_10\",runlr)\n",
    "    model = RNNModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-1/RNN_class_{runlr}\",\"neg\",runlr)\n",
    "\n",
    "    model = TransformerModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-10/Transformer_class_{runlr}\",\"neg_10\",runlr)\n",
    "    model = TransformerModel().cuda()\n",
    "    train_model(model,f\"Model_checkpoint-1/Transformer_class_{runlr}\",\"neg\",runlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "def predict(data,model,predictor,vote=True):\n",
    "    out = []\n",
    "    if \"CNN\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"CNN\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"CNN\"](data).unsqueeze(0))\n",
    "    if \"RCNN\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RCNN\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"RCNN\"](data).unsqueeze(0))\n",
    "    if \"RNN\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RNN\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:        \n",
    "            out.append(predictor[\"RNN\"](data).unsqueeze(0))\n",
    "    if \"RNN_atten\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RNN_atten\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:        \n",
    "            out.append(predictor[\"RNN_atten\"](data).unsqueeze(0))\n",
    "    if \"Transformer\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"Transformer\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"Transformer\"](data).unsqueeze(0))\n",
    "    if \"CNN_10-fold\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"CNN_10-fold\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"CNN_10-fold\"](data).unsqueeze(0))\n",
    "    if \"RCNN_10-fold\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RCNN_10-fold\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"RCNN_10-fold\"](data).unsqueeze(0))\n",
    "    if \"RNN_10-fold\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RNN_10-fold\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:        \n",
    "            out.append(predictor[\"RNN_10-fold\"](data).unsqueeze(0))\n",
    "    if \"RNN_atten_10-fold\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"RNN_atten_10-fold\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:        \n",
    "            out.append(predictor[\"RNN_atten_10-fold\"](data).unsqueeze(0))\n",
    "    if \"Transformer_10-fold\" in model:\n",
    "        if vote:\n",
    "            output = predictor[\"Transformer_10-fold\"](data)\n",
    "            predic = torch.max(output.data, 1)[1].cpu().unsqueeze(0)\n",
    "            out.append(predic)\n",
    "        else:\n",
    "            out.append(predictor[\"Transformer_10-fold\"](data).unsqueeze(0))\n",
    "    if vote:\n",
    "        out = (torch.concat(out).sum(0) > int(len(model)/2 -1)).int()\n",
    "    else:\n",
    "        out = torch.concat(out).mean(0)\n",
    "        out = torch.max(out.data, 1)[1].cpu().unsqueeze(0).numpy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the best lr for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_lr = {\n",
    "    \"CNN\":None,\n",
    "    \"RCNN\":None,\n",
    "    \"RNN\":None,\n",
    "    \"RNN_atten\":None,\n",
    "    \"Transformer\":None,\n",
    "    \"CNN_10-fold\":None,\n",
    "    \"RCNN_10-fold\":None,\n",
    "    \"RNN_10-fold\":None,\n",
    "    \"RNN_atten_10-fold\":None,\n",
    "    \"Transformer_10-fold\":None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = {\n",
    "    \"CNN\":CNNModel(),\n",
    "    \"RCNN\":RCNNModel(),\n",
    "    \"RNN\":RNNModel(),\n",
    "    \"RNN_atten\":RNN_attenModel(),\n",
    "    \"Transformer\":TransformerModel(),\n",
    "    \"CNN_10-fold\":CNNModel(),\n",
    "    \"RCNN_10-fold\":RCNNModel(),\n",
    "    \"RNN_10-fold\":RNNModel(),\n",
    "    \"RNN_atten_10-fold\":RNN_attenModel(),\n",
    "    \"Transformer_10-fold\":TransformerModel(),\n",
    "}\n",
    "for key in predictor.keys():\n",
    "    if \"_10-fold\" in key:\n",
    "        key_name = key.replace(\"_10-fold\",\"\")\n",
    "        model_path = f\"./Model_checkpoint-10/{key_name}_class_{best_run_lr[key]}/best_model.pth\"\n",
    "    else:\n",
    "        key_name = key\n",
    "        model_path = f\"./Model_checkpoint-1/{key_name}_class_{best_run_lr[key]}/best_model.pth\"\n",
    "    predictor[key].load_state_dict(torch.load(model_path))\n",
    "    predictor[key].cuda()\n",
    "    predictor[key].eval()\n",
    "allmodel = [\"CNN\",\"RCNN\",\"RNN\",\"RNN_atten\",\"Transformer\",\"CNN_10-fold\",\"RCNN_10-fold\",\"RNN_10-fold\",\"RNN_atten_10-fold\",\"Transformer_10-fold\"]\n",
    "test_set = MyDataset_class(\"params/test/pos\",\"params/test/neg_10\")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "from itertools import combinations\n",
    "\n",
    "whethe_vote = [True,False,]\n",
    "for vote in whethe_vote:\n",
    "    best_acc = {\n",
    "    \"name\":None,\n",
    "    \"acc\":0,\n",
    "    }\n",
    "    best_auc = {\n",
    "    \"name\":None,\n",
    "    \"auc\":0,\n",
    "    }\n",
    "    best_amp_precision = {\n",
    "    \"name\":None,\n",
    "    \"amp_precision\":0,\n",
    "    }\n",
    "    with open(f\"./combine_vote_{vote}\",\"w\") as wf:\n",
    "        wf.write(f\"combine,acc,auc,amp_precision,amp_recall,amp_f1-score,namp_precision,namp_recall,namp_f1-score\\n\")\n",
    "        for i in range(2,11):\n",
    "            for model in combinations(allmodel,i):\n",
    "                predict_all = np.array([], dtype=int)\n",
    "                labels_all = np.array([], dtype=int)\n",
    "                for i, (data, labels) in enumerate(test_loader):\n",
    "                    output = predict(data.cuda(),model,predictor,vote)\n",
    "                    labels_all = np.append(labels_all, labels)\n",
    "                    predict_all = np.append(predict_all, output)\n",
    "                test_acc = metrics.accuracy_score(labels_all,predict_all)\n",
    "                test_auc = metrics.roc_auc_score(labels_all,predict_all)\n",
    "                test_report = metrics.classification_report(labels_all, predict_all, target_names=[\"AMP\",\"NAMP\"], digits=4)\n",
    "                amp_list = test_report.split(\"\\n\")[2].split()\n",
    "                namp_list = test_report.split(\"\\n\")[3].split()\n",
    "                wf.write(f\"{model},{test_acc},{test_auc},{amp_list[1]},{amp_list[2]},{amp_list[3]},{namp_list[1]},{namp_list[2]},{namp_list[3]}\\n\")\n",
    "                if test_acc > best_acc[\"acc\"]:\n",
    "                    best_acc[\"acc\"] = test_acc\n",
    "                    best_acc[\"name\"] = model\n",
    "                if test_auc > best_auc[\"auc\"]:\n",
    "                    best_auc[\"auc\"] = test_auc\n",
    "                    best_auc[\"name\"] = model\n",
    "                if test_auc > float(amp_list[1]):\n",
    "                    best_amp_precision[\"amp_precision\"] = float(amp_list[1])\n",
    "                    best_amp_precision[\"name\"] = model\n",
    "    print(f\"******************************\")\n",
    "    print(vote)\n",
    "    print(best_acc)\n",
    "    print(best_auc)\n",
    "    print(best_amp_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Filiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq_path = \"pos_generate\"\n",
    "model = ['RCNN_10-fold', 'RNN_atten_10-fold', 'Transformer_10-fold']  #select best combine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from src.Transformer import peptide_tokenizer, encode_seq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyDataset_pred(Dataset):\n",
    "    def __init__(self,file_path):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(file_path,header=None)\n",
    "        self.num = len(self.data)\n",
    "        self.data = self.data.values.tolist()\n",
    "        self.max_length = 64\n",
    "    def __len__(self):\n",
    "        return  self.num\n",
    "    def __getitem__(self, index):\n",
    "        # AMP 0 NAMP 1\n",
    "        seq = self.data[index]\n",
    "        seq_list = peptide_tokenizer(seq[0])\n",
    "        encoded_seq = encode_seq(seq_list, self.max_length - 1, w2i)\n",
    "        encoded_seq = [0] + encoded_seq\n",
    "        assert len(encoded_seq) == 64, print(seq)\n",
    "        return torch.tensor(encoded_seq),seq[0]\n",
    "\n",
    "predictor = {\n",
    "    \"CNN\":CNNModel(),\n",
    "    \"RCNN\":RCNNModel(),\n",
    "    \"RNN\":RNNModel(),\n",
    "    \"RNN_atten\":RNN_attenModel(),\n",
    "    \"Transformer\":TransformerModel(),\n",
    "    \"CNN_10-fold\":CNNModel(),\n",
    "    \"RCNN_10-fold\":RCNNModel(),\n",
    "    \"RNN_10-fold\":RNNModel(),\n",
    "    \"RNN_atten_10-fold\":RNN_attenModel(),\n",
    "    \"Transformer_10-fold\":TransformerModel(),\n",
    "}\n",
    "for key in predictor.keys():\n",
    "    if \"_10-fold\" in key:\n",
    "        key_name = key.replace(\"_10-fold\",\"\")\n",
    "        model_path = f\"./Model_checkpoint-10/{key_name}_class/best_model.pth\"\n",
    "    else:\n",
    "        key_name = key\n",
    "        model_path = f\"./Model_checkpoint-1/{key_name}_class/best_model.pth\"\n",
    "    predictor[key].load_state_dict(torch.load(model_path))\n",
    "    predictor[key].cuda()\n",
    "    predictor[key].eval()\n",
    "\n",
    "test_set = MyDataset_pred(generate_seq_path)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "pos_seq = []\n",
    "for i, (data, seqs) in enumerate(test_loader):\n",
    "    output = predict(data.cuda(),model,predictor)\n",
    "    indices = np.where(output == 0)[0]\n",
    "    tem = np.array(seqs)[indices].tolist()\n",
    "    # tem = np.array(seqs)[np.where(output == 0)].tolist()\n",
    "    pos_seq.extend(tem)\n",
    "pos_seq = list(set(pos_seq))\n",
    "print(len(pos_seq))\n",
    "with open(f\"./pos_generate_filiter\",\"w\") as wf:\n",
    "    for seq in pos_seq:\n",
    "        wf.write(f\"{seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_konw(data, know_data):\n",
    "    same_data = pd.merge(data, know_data, how=\"inner\")\n",
    "    outdata = data[~data.isin(same_data)].dropna()\n",
    "    return outdata\n",
    "def remove_c(data):\n",
    "    data = data[~data[0].str.contains(\"C\")]\n",
    "    return data\n",
    "def seq_check_pos_charge(seq):\n",
    "    for i in range(len(seq)-4):\n",
    "        tem_seq = seq[i:i+5]\n",
    "        num = tem_seq.count(\"K\") + tem_seq.count(\"R\")\n",
    "        if num > 3:\n",
    "            return False\n",
    "    return True\n",
    "def remove_pos_charge(data):\n",
    "    data = data[data[0].apply(seq_check_pos_charge)]\n",
    "    return data\n",
    "def seq_check_hydrophobic(seq):\n",
    "    for i in range(len(seq)-2):\n",
    "        tem_seq = seq[i:i+3]\n",
    "        num = tem_seq.count(\"F\") + tem_seq.count(\"V\") + \\\n",
    "              tem_seq.count(\"I\") + tem_seq.count(\"W\") + \\\n",
    "              tem_seq.count(\"L\") + tem_seq.count(\"A\") + \\\n",
    "              tem_seq.count(\"M\")\n",
    "        if num == 3:\n",
    "            return False\n",
    "    return True\n",
    "def remove_hydrophobic(data):\n",
    "    data = data[data[0].apply(seq_check_hydrophobic)]\n",
    "    return data\n",
    "def seq_check_repeat_three(seq):\n",
    "    for i in range(len(seq)-2):\n",
    "        tem_seq = seq[i:i+3]\n",
    "        # print(tem_seq)g \n",
    "        if tem_seq[0] == tem_seq[1] and tem_seq[0] == tem_seq[2]:\n",
    "            return False\n",
    "    return True\n",
    "def remove_repeat_three(data):\n",
    "    data = data[data[0].apply(seq_check_repeat_three)]\n",
    "    return data\n",
    "\n",
    "\n",
    "data = pd.read_csv(f\"pos_generate_filiter\",header=None)\n",
    "know_data = pd.read_csv(f\"params/pos_data\",header=None)\n",
    "data = remove_konw(data,know_data)\n",
    "data = remove_c(data)\n",
    "data = remove_pos_charge(data)\n",
    "data = remove_hydrophobic(data)\n",
    "data = remove_repeat_three(data)\n",
    "data.to_csv(f\"./finally_filiter\",header= False,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        id_counter = 1\n",
    "        for line in infile:\n",
    "            sequence = line.strip()\n",
    "            fasta_entry = f\">seq_{id_counter}\\n{sequence}\\n\"\n",
    "            outfile.write(fasta_entry)\n",
    "            id_counter += 1\n",
    "\n",
    "input_file = 'finally_filiter'\n",
    "output_file = 'finally_filiter.fasta'\n",
    "convert_to_fasta(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"cd-hit -i finally_filiter.fasta -c 0.6 -o cluster_res -n 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run \"python prepare_cgmd.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_folder = 'cgmd_train'\n",
    "destination_folder = 'cgmd_train_pdb'\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdb'):\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            destination_file_path = os.path.join(destination_folder, file)\n",
    "            shutil.copy2(source_file_path, destination_file_path)\n",
    "\n",
    "source_folder = 'cgmd_predict'\n",
    "destination_folder = 'cgmd_predict_pdb'\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdb'):\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            destination_file_path = os.path.join(destination_folder, file)\n",
    "            shutil.copy2(source_file_path, destination_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "file_list = os.listdir(f\"./cgmd_predict_pdb\") \n",
    "os.makedirs(f\"./runmd_predict\")\n",
    "pmd = os.getcwd()\n",
    "for file in file_list:\n",
    "    file_path = f\"{pmd}/cgmd_predict_pdb/{file}\"\n",
    "    os.makedirs(f\"{pmd}/runmd_predict/{file}\",exist_ok=True)\n",
    "    shutil.copyfile(file_path,f\"{pmd}/runmd_predict/{file}/peptide.pdb\")\n",
    "    os.system(f\"{pmd}/martinize.py -f {pmd}/runmd_predict/{file}/peptide.pdb -o {pmd}/runmd_predict/{file}/system.top -x {pmd}/runmd_predict/{file}/peptide_cg.pdb -dssp /usr/bin/dssp -p backbone -ff martini22\")\n",
    "    shutil.copyfile(f\"{pmd}/Protein.itp\",f\"./runmd_predict/{file}/Protein.itp\")\n",
    "    os.system(f\"~/anaconda3/envs/py2.7/bin/python {pmd}/insane.py -f {pmd}/runmd_predict/{file}/peptide_cg.pdb -o {pmd}/runmd_predict/{file}/system.gro -p {pmd}/runmd_predict/{file}/system.top -pbc square -box 8,8,12 -sol W:90 -sol WF:10 -l POPC:3 -l POPG:1 -salt 0.15 -dm 4\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.2.itp\",f\"./runmd_predict/{file}/martini_v2.2.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_POPG_02.itp\",f\"{pmd}/runmd_predict/{file}/martini_v2.0_POPG_02.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_POPC_02.itp\",f\"{pmd}/runmd_predict/{file}/martini_v2.0_POPC_02.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_ions.itp\",f\"{pmd}/runmd_predict/{file}/martini_v2.0_ions.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/equilibration.mdp\",f\"{pmd}/runmd_predict/{file}/equilibration.mdp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/minimization.mdp\",f\"{pmd}/runmd_predict/{file}/minimization.mdp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/production.mdp\",f\"{pmd}/runmd_predict/{file}/production.mdp\")\n",
    "    with open(f\"{pmd}/runmd_predict/{file}/run.sh\",\"w\") as wf:\n",
    "        wf.write(f\"#!/bin/bash\\n\")\n",
    "        wf.write(f\"source ~/Software/gromacs/bin/GMXRC\\n\")\n",
    "        wf.write(f\"gmx grompp -f minimization.mdp -c system.gro -p system_run.top -o minimization.tpr\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm minimization -v\\n\")\n",
    "        wf.write(f\"gmx grompp -f equilibration.mdp -c minimization.gro -p system_run.top -o equilibration.tpr -n index.ndx\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm equilibration -v\\n\")\n",
    "        wf.write(f\"gmx grompp -f production.mdp -c equilibration.gro -p system_run.top -o dynamic.tpr -n index.ndx\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm dynamic -v\\n\")\n",
    "\n",
    "    with open(f\"{pmd}/runmd_predict/{file}/system.top\") as rf:\n",
    "        lines = rf.readlines()\n",
    "        with open(f\"{pmd}/runmd_predict/{file}/system_run.top\",\"w\") as wf:\n",
    "            wf.write(f\"#include \\\"martini_v2.2.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_POPC_02.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_POPG_02.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_ions.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"Protein.itp\\\"\\n\")\n",
    "\n",
    "            for i, line in enumerate(lines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                wf.write(line)\n",
    "    pop = os.popen(f\"gmx make_ndx -f {pmd}/runmd_predict/{file}/system.gro -o {pmd}/runmd_predict/{file}/index.ndx\\n\",\"w\")\n",
    "    \n",
    "    pop.write(f\"13|14\\n\")\n",
    "    pop.write(f\"15|16|19\\n\")\n",
    "    pop.write(f\"q\\n\")\n",
    "    pop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "file_list = os.listdir(f\"./cgmd_train_pdb\") \n",
    "os.makedirs(f\"./runmd\")\n",
    "pmd = os.getcwd()\n",
    "for file in file_list:\n",
    "    file_path = f\"{pmd}/cgmd_train_pdb/{file}\"\n",
    "    os.makedirs(f\"{pmd}/runmd/{file}\",exist_ok=True)\n",
    "    shutil.copyfile(file_path,f\"{pmd}/runmd/{file}/peptide.pdb\")\n",
    "    os.system(f\"{pmd}/martinize.py -f {pmd}/runmd/{file}/peptide.pdb -o {pmd}/runmd/{file}/system.top -x {pmd}/runmd/{file}/peptide_cg.pdb -dssp /usr/bin/dssp -p backbone -ff martini22\")\n",
    "    shutil.copyfile(f\"{pmd}/Protein.itp\",f\"./runmd/{file}/Protein.itp\")\n",
    "    os.system(f\"/home/wang/anaconda3/envs/py2.7/bin/python {pmd}/insane.py -f {pmd}/runmd/{file}/peptide_cg.pdb -o {pmd}/runmd/{file}/system.gro -p {pmd}/runmd/{file}/system.top -pbc square -box 8,8,12 -sol W:90 -sol WF:10 -l POPC:3 -l POPG:1 -salt 0.15 -dm 4\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.2.itp\",f\"./runmd/{file}/martini_v2.2.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_POPG_02.itp\",f\"{pmd}/runmd/{file}/martini_v2.0_POPG_02.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_POPC_02.itp\",f\"{pmd}/runmd/{file}/martini_v2.0_POPC_02.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/md_test/martini_v2.0_ions.itp\",f\"{pmd}/runmd/{file}/martini_v2.0_ions.itp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/equilibration.mdp\",f\"{pmd}/runmd/{file}/equilibration.mdp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/minimization.mdp\",f\"{pmd}/runmd/{file}/minimization.mdp\")\n",
    "    shutil.copyfile(f\"{pmd}/mdp_file/production.mdp\",f\"{pmd}/runmd/{file}/production.mdp\")\n",
    "    with open(f\"{pmd}/runmd/{file}/run.sh\",\"w\") as wf:\n",
    "        wf.write(f\"#!/bin/bash\\n\")\n",
    "        wf.write(f\"source ~/Software/gromacs/bin/GMXRC\\n\")\n",
    "        wf.write(f\"gmx grompp -f minimization.mdp -c system.gro -p system_run.top -o minimization.tpr\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm minimization -v\\n\")\n",
    "        wf.write(f\"gmx grompp -f equilibration.mdp -c minimization.gro -p system_run.top -o equilibration.tpr -n index.ndx\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm equilibration -v\\n\")\n",
    "        wf.write(f\"gmx grompp -f production.mdp -c equilibration.gro -p system_run.top -o dynamic.tpr -n index.ndx\\n\")\n",
    "        wf.write(f\"gmx mdrun -deffnm dynamic -v\\n\")\n",
    "\n",
    "    with open(f\"{pmd}/runmd/{file}/system.top\") as rf:\n",
    "        lines = rf.readlines()\n",
    "        with open(f\"{pmd}/runmd/{file}/system_run.top\",\"w\") as wf:\n",
    "            wf.write(f\"#include \\\"martini_v2.2.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_POPC_02.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_POPG_02.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"martini_v2.0_ions.itp\\\"\\n\")\n",
    "            wf.write(f\"#include \\\"Protein.itp\\\"\\n\")\n",
    "\n",
    "            for i, line in enumerate(lines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                wf.write(line)\n",
    "    pop = os.popen(f\"gmx make_ndx -f {pmd}/runmd/{file}/system.gro -o {pmd}/runmd/{file}/index.ndx\\n\",\"w\")\n",
    "    \n",
    "    pop.write(f\"13|14\\n\")\n",
    "    pop.write(f\"15|16|19\\n\")\n",
    "    pop.write(f\"q\\n\")\n",
    "    pop.close()\n",
    "os.chdir(pmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CGMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"runmd\"\n",
    "file_list = os.listdir(f\"./{file_name}\")\n",
    "path = os.getcwd()\n",
    "for file in file_list:\n",
    "    if os.path.exists(f\"{path}/{file_name}/{file}/have_run\"):\n",
    "        print(f\"{path}/{file_name}/{file} have Run!\")\n",
    "        continue\n",
    "    else:\n",
    "        os.mkdir(f\"{path}/{file_name}/{file}/have_run\")\n",
    "        os.chdir(f\"{path}/{file_name}/{file}\")\n",
    "        os.system(f\"sh run.sh\")\n",
    "        print(f\"****************************\")\n",
    "        print(f\"MDRUN : {path}/runmd/{file}\")\n",
    "file_name = \"runmd_predict\"\n",
    "file_list = os.listdir(f\"./{file_name}\")\n",
    "path = os.getcwd()\n",
    "for file in file_list:\n",
    "    if os.path.exists(f\"{path}/{file_name}/{file}/have_run\"):\n",
    "        print(f\"{path}/{file_name}/{file} have Run!\")\n",
    "        continue\n",
    "    else:\n",
    "        os.mkdir(f\"{path}/{file_name}/{file}/have_run\")\n",
    "        os.chdir(f\"{path}/{file_name}/{file}\")\n",
    "        os.system(f\"sh run.sh\")\n",
    "        print(f\"****************************\")\n",
    "        print(f\"MDRUN : {path}/runmd/{file}\")\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "floder_list = os.listdir(f\"{path}/runmd\")\n",
    "for floder in floder_list:\n",
    "    floder_path = f\"{path}/runmd/{floder}\"\n",
    "    os.chdir(floder_path)\n",
    "    try:\n",
    "        os.makedirs(f\"{path}/runmd/{floder}/whole\")\n",
    "        pop = os.popen(f\"gmx trjconv -s dynamic.tpr -f dynamic.xtc -o dynamic_whole.xtc -pbc whole\",\"w\")\n",
    "        pop.write(f\"0\\n\")\n",
    "        pop.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "floder_list = os.listdir(f\"{path}/runmd_predict\")\n",
    "for floder in floder_list:\n",
    "    floder_path = f\"{path}/runmd_predict/{floder}\"\n",
    "    os.chdir(floder_path)\n",
    "    try:\n",
    "        os.makedirs(f\"{path}/runmd_predict/{floder}/whole\")\n",
    "        pop = os.popen(f\"gmx trjconv -s dynamic.tpr -f dynamic.xtc -o dynamic_whole.xtc -pbc whole\",\"w\")\n",
    "        pop.write(f\"0\\n\")\n",
    "        pop.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDAnalysis import Universe\n",
    "from MDAnalysis.analysis import distances\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_line(gro_path,xtc_path,save_path_data):\n",
    "    product = Universe(gro_path,xtc_path,)\n",
    "    peptide = product.select_atoms(f\"protein\")\n",
    "    member_heads = product.select_atoms(f\"resname POPC or resname POPG\")\n",
    "    all_list = []\n",
    "    for ts in product.trajectory:\n",
    "        out = distances.distance_array(peptide,member_heads)\n",
    "        out_numpy = np.array(out).min(-1)\n",
    "        all_list.append(out_numpy.tolist())\n",
    "    data = np.array(all_list)\n",
    "    name = []\n",
    "    for atom in peptide:\n",
    "        name.append(f\"{atom.resname}_{atom.id}\")\n",
    "    data_T = data.T\n",
    "    np.save(save_path_data,data_T)\n",
    "\n",
    "os.makedirs(f\"./out_whole\",exist_ok=True)\n",
    "os.makedirs(f\"./out_whole/data\",exist_ok=True)\n",
    "\n",
    "floder_list = os.listdir(f\"./runmd\")\n",
    "path = os.getcwd()\n",
    "for floder in tqdm(floder_list):\n",
    "    floder_path = f\"{path}/runmd/{floder}\"\n",
    "    try:\n",
    "        os.makedirs(f\"{path}/runmd/{floder}/analysis_whole\")\n",
    "        print(f\"analysis {floder}\")\n",
    "        get_line(f\"{floder_path}/dynamic.gro\",f\"{floder_path}/dynamic_whole.xtc\",f\"./out_whole/data/{floder}.npy\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "os.makedirs(f\"./predict\",exist_ok=True)\n",
    "os.makedirs(f\"./predict/data\",exist_ok=True)\n",
    "\n",
    "floder_list = os.listdir(f\"./runmd_predict\")\n",
    "path = os.getcwd()\n",
    "for floder in tqdm(floder_list):\n",
    "    floder_path = f\"{path}/runmd_predict/{floder}\"\n",
    "    try:\n",
    "        os.makedirs(f\"{path}/runmd_predict/{floder}/analysis_whole\")\n",
    "        print(f\"analysis {floder}\")\n",
    "        get_line(f\"{floder_path}/dynamic.gro\",f\"{floder_path}/dynamic_whole.xtc\",f\"./predict/data/{floder}.npy\")\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "file_list = os.listdir(f\"./out/data\")\n",
    "for file in file_list:\n",
    "    temdata = np.load(f\"./out/data/{file}\").mean(0).tolist()\n",
    "    data.append(temdata)\n",
    "    aname = file.split(\".\")[0]\n",
    "    if \"pos\" in aname:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "data = np.array(data)\n",
    "label = np.array(label)\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,label,test_size=0.2)\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "parameters = {'n_estimators': range(50,500,50),'max_depth':range(3,15,2),\n",
    "                'min_samples_leaf':[5,6,7],'max_features':[1,2,3,4,5,6]}\n",
    "grid_rfc = GridSearchCV(rfc,parameters,scoring='f1_macro',n_jobs=-1)\n",
    "grid_rfc.fit(X_train,y_train)\n",
    "print(grid_rfc.best_params_,grid_rfc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,max_depth=9,max_features=5,min_samples_leaf=7,bootstrap=True,criterion=\"entropy\")\n",
    "rfc.fit(X_train,y_train)\n",
    "pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(f\"./out_generate/data\")\n",
    "pred_proba = []\n",
    "pred_name = []\n",
    "with open(f\"./filiter_out_new\",\"w\") as wf:\n",
    "    for file in file_list:\n",
    "        adata = np.load(f\"./out_generate/data/{file}\").mean(0)\n",
    "        adata = np.expand_dims(adata,axis=0)\n",
    "        if adata.shape[1] != 25001:\n",
    "            print(file)\n",
    "            continue\n",
    "        wf.write(f\"{file}\\t{rfc.predict_proba(adata)[:,0]}\\n\")\n",
    "        pred_proba.append(rfc.predict_proba(adata)[:,0])\n",
    "        pred_name.append(file)\n",
    "pred_proba_np = np.array(pred_proba).squeeze()\n",
    "argsort_np = pred_proba_np.argsort()\n",
    "with open(f\"./filiter_out_new_sort\",\"w\") as wf:\n",
    "    for i in argsort_np:\n",
    "        wf.write(f\"{pred_name[i]}\\t\\t{pred_proba[i][0]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mdanalysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbe55e08f0e5d9c77c735017de6c7257da0b6fb8c402d130e0f3e100e08593f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
